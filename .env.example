# AI Model Configuration
# Choose your AI provider: "ollama" (local), "openai", or "anthropic"
AI_PROVIDER=ollama

# Hybrid Provider Setup (Optional)
# Use a separate provider for embeddings only (e.g., use Ollama for Q&A but Anthropic for embeddings)
# If not set, will use AI_PROVIDER for both
# Example: EMBEDDING_PROVIDER=anthropic (uses Anthropic for embeddings, Ollama for Q&A)
# EMBEDDING_PROVIDER=anthropic

# Ollama Configuration (for local open-source models)
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama3.2:latest
# Ollama embedding model (used when AI_PROVIDER or EMBEDDING_PROVIDER is ollama)
EMBEDDING_MODEL=nomic-embed-text

# OpenAI Configuration (optional)
# OPENAI_API_KEY=your_openai_api_key
# OPENAI_MODEL=gpt-4-turbo-preview
# OpenAI embedding model (used when EMBEDDING_PROVIDER is openai)
# EMBEDDING_MODEL=text-embedding-3-small

# Anthropic Configuration (optional)
# ANTHROPIC_API_KEY=your_anthropic_api_key
# ANTHROPIC_MODEL=claude-3-sonnet-20240229
# Note: Anthropic doesn't provide embeddings directly
# For embeddings, use EMBEDDING_PROVIDER=openai with a separate OpenAI API key
# Recommended setup: AI_PROVIDER=ollama (free Q&A) + EMBEDDING_PROVIDER=openai (reliable embeddings)

# Documentation Sources
SUSE_DOCS_BASE_URL=https://documentation.suse.com
RANCHER_DOCS_URL=https://ranchermanager.docs.rancher.com
K3S_DOCS_URL=https://docs.k3s.io

# Vector Database Configuration
VECTOR_DB_PATH=./data/vectors
EMBEDDING_MODEL=nomic-embed-text

# Caching Configuration
EMBEDDING_CACHE_PATH=./data/embedding-cache.json
# SQLite page cache (default, recommended) - much faster and more efficient for large document sets
PAGE_CACHE_PATH=./data/page-cache.db
# Set USE_JSON_CACHE=true to use legacy JSON cache (not recommended for >100 documents)
# USE_JSON_CACHE=false
HTML_CACHE_DIR=./data/html

# Performance Tuning
# Number of documents to process in parallel (default: 1, max recommended: 3)
# Start with 1 for stability, increase gradually
FETCH_BATCH_SIZE=1
# Delay between document batches in milliseconds (default: 500)
BATCH_DELAY=500
# Number of embeddings to generate concurrently (default: 1, increase cautiously)
# Lower values = more stable, higher values = faster but may overwhelm Ollama
# Use 1 for sequential processing (most stable)
EMBEDDING_CONCURRENCY=1
# Delay between embedding requests in milliseconds (default: 250)
# Increase if experiencing connection drops
EMBEDDING_DELAY=250
# Text chunk size for embeddings (default: 1500, max recommended: 2000)
# Larger chunks = fewer embeddings = faster, but may cause Ollama connection issues
# Windows systems may need lower values (1000-1500) due to TCP buffer limits
CHUNK_SIZE=1500

# Server Configuration
MCP_SERVER_NAME=docs-navigator-suse
LOG_LEVEL=info
